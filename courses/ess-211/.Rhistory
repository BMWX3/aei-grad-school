g = cv.err(data, 10)
g
View(data)
K <- seq(2, nrow(data))
K
K <- seq(2, nrow(data)/2)
K
append(K, nrow(data))
K
?se
g
K <- seq(2, nrow(data)/2)
# then append a final value to the vector, the number of rows, to use for n-fold x-val
K <- append(K, nrow(data))
# create a matrix to store outputs of mean/stdev for test/training results
testMatrix <- matrix(ncol = 2, nrow = length(K))
trainMatrix <- matrix(ncol = 2, nrow = length(K))
# loop through each of the k-fold methods and calculate mean/stdev test/training error
for (i in length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[1])
trainMatrix[i,2] <- sd(ifoldMatrix[1])
testMatrix[i,1] <- mean(ifoldMatrix[1])
trainMatrix[i,2] <- sd(ifoldMatrix[1])
}
testMatrix
K
K[1]
ifoldMatrix
ifoldMatrix[1]
ifoldMatrix[1,]
ifoldMatrix[,1]
for (i in length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[,1])
trainMatrix[i,2] <- sd(ifoldMatrix[,1])
testMatrix[i,1] <- mean(ifoldMatrix[,2])
trainMatrix[i,2] <- sd(ifoldMatrix[,2])
}
trainMatrix
i
#  nrows/2 (two samples per hold-out)
K <- seq(2, nrow(data)/2)
# then append a final value to the vector, the number of rows, to use for n-fold x-val
K <- append(K, nrow(data))
# create a matrix to store outputs of mean/stdev for test/training results
testMatrix <- matrix(ncol = 2, nrow = length(K))
trainMatrix <- matrix(ncol = 2, nrow = length(K))
# loop through each of the k-fold methods and calculate mean/stdev test/training error
for (i in length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[,1])
trainMatrix[i,2] <- sd(ifoldMatrix[,1])
testMatrix[i,1] <- mean(ifoldMatrix[,2])
trainMatrix[i,2] <- sd(ifoldMatrix[,2])
}
trainMatrix
K[i]
ifoldMatrix <- cv.err(data, 10)
ifoldMatrix
mean(ifoldMatrix[,1])
mean(ifoldMatrix[,2])
source("ESS-211-Functions.R")
# read the input data for assignment 4
data <- read.csv(file = "course_material/exercise3.data.csv", header = TRUE)
#############################
# task 1 - cross validation error on k splits
# write a function to calculate rmse for k splits
cv.err <- function(data.frame, K){
# split the data into ~equal chunks
#  first, find the length of the data frame to split
nr <- nrow(data.frame)
# split the data using the cut funtion
groups <- cut(1:nr, K, label = FALSE)
# create a matrix that will contain the 2 x K training and test errors
errorMatrix <- matrix(ncol = 2, nrow = K)
# loop through each split and calculate training and test errors
for (i in 1:K){
errorMatrix[i,] <- rmse(data.frame[groups != i,], data.frame[groups == i,])
}
# return the matrix
return(errorMatrix)
}
#############################
# task 3 - varying k
# set up a vector for the different values of k we'll test, from 2 (50% hold-out) to
#  nrows/2 (two samples per hold-out)
K <- seq(2, nrow(data)/2)
# then append a final value to the vector, the number of rows, to use for n-fold x-val
K <- append(K, nrow(data))
# create a matrix to store outputs of mean/stdev for test/training results
testMatrix <- matrix(ncol = 2, nrow = length(K))
trainMatrix <- matrix(ncol = 2, nrow = length(K))
# loop through each of the k-fold methods and calculate mean/stdev test/training error
for (i in length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[,1])
trainMatrix[i,2] <- sd(ifoldMatrix[,1])
testMatrix[i,1] <- mean(ifoldMatrix[,2])
trainMatrix[i,2] <- sd(ifoldMatrix[,2])
}
trainMatrix
for (i in 1:length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[,1])
trainMatrix[i,2] <- sd(ifoldMatrix[,1])
testMatrix[i,1] <- mean(ifoldMatrix[,2])
trainMatrix[i,2] <- sd(ifoldMatrix[,2])
}
trainMatrix
testMatrix
ifoldMatrix
sd(ifoldMatrix[,2])
for (i in 1:length(K)){
# run the k-fold xval procedure on the i'th value of K
ifoldMatrix <- cv.err(data, K[i])
# assign mean and stdev of each xval procedure to storage matrices
trainMatrix[i,1] <- mean(ifoldMatrix[,1])
trainMatrix[i,2] <- sd(ifoldMatrix[,1])
testMatrix[i,1] <- mean(ifoldMatrix[,2])
testMatrix[i,2] <- sd(ifoldMatrix[,2])
}
testMatrix
trainMatrix
library(gplots)
plotCI
?plotCI
plot(K,trainMatrix[,1])
plot(K,trainMatrix[,2])
plot(K,testMatrix[,1])
plot(K,testMatrix[,2])
testMatrix[i,2]
testMatrix[,2]
testMatrix[,2] / sqrt(K)
plot((K / nrow(data)),testMatrix[,2])
plot(1/(K / nrow(data)),testMatrix[,2])
plot(1/(K / nrow(data)),testMatrix[,2],xlab = "% data included in k-fold x-val")
plot(1/(K / nrow(data)),testMatrix[,1],xlab = "% data included in k-fold x-val")
ymin <- min(c(testMatrix[,1], trainMatrix[,1]))
ymax <- max(c(testMatrix[,1], trainMatrix[,1]))
ymin - ymax
ymax - ymin
ymax - ymin / 10
(ymax - ymin) / 10
1 / K
20 / K
plot(nrow(data)/K,testMatrix[,1],xlab = "% data included in k-fold x-val")
barplot(testMatrix[,1])
?plotCI
title = ""
ylab = "RMSE"
xlab <- "# samples per fold"
# set min/max limits
ymin <- min(c(testMatrix[,1], trainMatrix[,1]))
ymax <- max(c(testMatrix[,1], trainMatrix[,1]))
buffer <- (ymax - ymin) / 10
ylim <- c(ymin-buffer, ymax+buffer)
plot(nrow(data)/K,testMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim)
plot(nrow(data)/K,testMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, psym=19)
plot(nrow(data)/K,testMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plotCI(xvals, trainMatrix[,1], type = 'l', uiw = (trainMatrix[,1] + trainMatrix[,2]),
liw = (trainMatrix[,1], - trainMatrix[,2]))
# add error bars
plotCI(xvals, trainMatrix[,1], type = 'l', uiw = (trainMatrix[,1] + trainMatrix[,2]),
liw = (trainMatrix[,1] - trainMatrix[,2]))
xvals <- nrow(data)/K
plotCI(xvals, trainMatrix[,1], type = 'l', uiw = (trainMatrix[,1] + trainMatrix[,2]),
liw = (trainMatrix[,1] - trainMatrix[,2]))
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = (trainMatrix[,1] + trainMatrix[,2]),
liw = (trainMatrix[,1] - trainMatrix[,2]))
plot(xvals,trainMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plot(xvals,test
Matrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plot(xvals,testMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plot(xvals,trainMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plot(xvals,testMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = (trainMatrix[,1] + trainMatrix[,2]),
liw = (trainMatrix[,1] - trainMatrix[,2]), pch = 19)
trainMatrix[2]
trainMatrix[,2]
trainMatrix[,1] + trainMatrix[,2]
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2],
liw = trainMatrix[,2], pch = 19)
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "# samples per fold"
# set up x axis as the number of training samples per fold
xvals <- nrow(data)/K
# set up the training data plot
plot(xvals,trainMatrix[,1],xlab = xlab, ylab = ylab, ylim=ylim, pch=19)
# add error bars
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle)
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
K
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
xlab <- "N folds"
# set up x axis as the number of training samples per fold
xvals <- nrow(data)/K
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
#
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'blue', barcol = 'yellow')
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'orange', barcol = 'black')
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'teal')
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
par(mfcol=c(1,3))
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
par(mfcol=c(1,2))
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2],
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
plotCI(xvals, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2],
)
testMatrix[,2]
sqrt(K)
source('~/cba/aei-grad-school/courses/ess-211/Assignment4-CrossValidation.R')
source('~/cba/aei-grad-school/courses/ess-211/Assignment4-CrossValidation.R')
source('~/cba/aei-grad-school/courses/ess-211/Assignment4-CrossValidation.R')
source('~/cba/aei-grad-school/courses/ess-211/Assignment4-CrossValidation.R')
K
trainMatrix
sqrt(K)
testMatrix
ymin <- min(c(trainMatrix[,1]-trainMatrix[,2], testMatrix[,1]-testMatrix[,2]))
ymax <- max(c(trainMatrix[,1]+trainMatrix[,2], testMatrix[,1]+testMatrix[,2]))
buffer <- (ymax - ymin) * 0.1
ylim <- c(ymin-buffer, ymax+buffer)
# plot the means and confidence intervals of the training and test error with varying K
#  first, set up titles, etc.
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "N folds"
# set up x axis as the number of training samples per fold
xvals <- nrow(data)/K
par(mfcol=c(1,2))
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
data[,5:7] <- rnorm(3 * nrow(data))
data
data[,1]
data[,1:2]
ncol(data)
2:2
length(2:2)
length(2:3)
length(2:4)
K[1]
K[-1]
K[K]
K
nrow(data)
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i,2] <- sd(columnMatrix[,2])
}
# add three columnds of noise to the data
data[,5:7] <- rnorm(3 * nrow(data))
# create matrcesx to store outputs
noiseTrainMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
noiseTestMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
# iteratively add columns to the linear regression to see how cross validation errors change
#  with additional variables and noise
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i,2] <- sd(columnMatrix[,2])
}
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i,2] <- sd(columnMatrix[,2])
}
ncol(data)
2:2
2:7
i = 2
columnMatrix <- cv.err(data[,1:i], nrow(data))
columnMatrix
data[,1:i]
noiseTrainMatrix
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i-1,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i-1,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i-1,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i-1,2] <- sd(columnMatrix[,2])
}
noiseTestMatrix
noseTrainMatrix
noiseTrainMatrix
title('Training and test error as a functin of the number of folds', outer = TRUE)
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
# and add a title
title('Training and test error as a functin of the number of folds', outer = TRUE)
1:ncol(data)-1
noiseTrainMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
noiseTestMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
# iteratively add columns to the linear regression to see how cross validation errors change
#  with additional variables and noise
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i-1,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i-1,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i-1,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i-1,2] <- sd(columnMatrix[,2])
}
# plot the means and standard errors of the training and test error with varying number of terms to fit
#  first, set up titles, etc.
mainTitle <- "Training and test error as a function of the number of terms"
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "N terms fit"
# find the min and max ranges for all data to plot on the same axes
ymin <- min(c(noiseTrainMatrix[,1]-noiseTrainMatrix[,2], noiseTestMatrix[,1]-noiseTestMatrix[,2]))
ymax <- max(c(noiseTrainMatrix[,1]+noiseTrainMatrix[,2], noiseTestMatrix[,1]+noiseTestMatrix[,2]))
buffer <- (ymax - ymin) * 0.1
ylim <- c(ymin-buffer, ymax+buffer)
# set up plot structure
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(1:(ncol(data)-1), noiseTrainMatrix[,1], type = 'p', uiw = noiseTrainMatrix[,2], liw = noiseTrainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(1:(ncol(data)-1), noiseTestMatrix[,1], type = 'p', uiw = noiseTestMatrix[,2], liw = noiseTestMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
# and add a title
title(mainTitle, outer = TRUE)
noiseTrainMatrix
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(K, trainMatrix[,1], type = 'p', uiw = trainMatrix[,2], liw = trainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(K, testMatrix[,1], type = 'p', uiw = testMatrix[,2], liw = testMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
# and add a title
title(mainTitle, outer = TRUE)
# set up the training data plot
plotCI(1:(ncol(data)-1), noiseTrainMatrix[,1], type = 'p', uiw = noiseTrainMatrix[,2], liw = noiseTrainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(1:(ncol(data)-1), noiseTestMatrix[,1], type = 'p', uiw = noiseTestMatrix[,2], liw = noiseTestMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
?rnorm
data[,5:7] <- rnorm(3 * nrow(data), sd = 2)
# create matrcesx to store outputs
noiseTrainMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
noiseTestMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
# iteratively add columns to the linear regression to see how cross validation errors change
#  with additional variables and noise
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i-1,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i-1,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i-1,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i-1,2] <- sd(columnMatrix[,2])
}
# plot the means and standard errors of the training and test error with varying number of terms to fit
#  first, set up titles, etc.
mainTitle <- "Training and test error as a function of the number of terms"
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "N terms fit"
# find the min and max ranges for all data to plot on the same axes
ymin <- min(c(noiseTrainMatrix[,1]-noiseTrainMatrix[,2], noiseTestMatrix[,1]-noiseTestMatrix[,2]))
ymax <- max(c(noiseTrainMatrix[,1]+noiseTrainMatrix[,2], noiseTestMatrix[,1]+noiseTestMatrix[,2]))
buffer <- (ymax - ymin) * 0.1
ylim <- c(ymin-buffer, ymax+buffer)
# set up plot structure
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(1:(ncol(data)-1), noiseTrainMatrix[,1], type = 'p', uiw = noiseTrainMatrix[,2], liw = noiseTrainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(1:(ncol(data)-1), noiseTestMatrix[,1], type = 'p', uiw = noiseTestMatrix[,2], liw = noiseTestMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
# and add a title
title(mainTitle, outer = TRUE)
data[,5:7] <- rnorm(3 * nrow(data), sd = 3)
# create matrcesx to store outputs
noiseTrainMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
noiseTestMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
# iteratively add columns to the linear regression to see how cross validation errors change
#  with additional variables and noise
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i-1,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i-1,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i-1,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i-1,2] <- sd(columnMatrix[,2])
}
# plot the means and standard errors of the training and test error with varying number of terms to fit
#  first, set up titles, etc.
mainTitle <- "Training and test error as a function of the number of terms"
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "N terms fit"
# find the min and max ranges for all data to plot on the same axes
ymin <- min(c(noiseTrainMatrix[,1]-noiseTrainMatrix[,2], noiseTestMatrix[,1]-noiseTestMatrix[,2]))
ymax <- max(c(noiseTrainMatrix[,1]+noiseTrainMatrix[,2], noiseTestMatrix[,1]+noiseTestMatrix[,2]))
buffer <- (ymax - ymin) * 0.1
ylim <- c(ymin-buffer, ymax+buffer)
# set up plot structure
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(1:(ncol(data)-1), noiseTrainMatrix[,1], type = 'p', uiw = noiseTrainMatrix[,2], liw = noiseTrainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(1:(ncol(data)-1), noiseTestMatrix[,1], type = 'p', uiw = noiseTestMatrix[,2], liw = noiseTestMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
data[,5:7] <- rnorm(3 * nrow(data), sd = 1)
# create matrcesx to store outputs
noiseTrainMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
noiseTestMatrix <- matrix(ncol = 2, nrow = ncol(data)-1)
# iteratively add columns to the linear regression to see how cross validation errors change
#  with additional variables and noise
for (i in 2:ncol(data)){
# we're going to use the leave-one-out method to assess the contribution of additional variables/noise
columnMatrix <- cv.err(data[,1:i], nrow(data))
# assign mean/stdev to matrix
noiseTrainMatrix[i-1,1] <- mean(columnMatrix[,1])
noiseTrainMatrix[i-1,2] <- sd(columnMatrix[,1])
noiseTestMatrix[i-1,1] <- mean(columnMatrix[,2])
noiseTestMatrix[i-1,2] <- sd(columnMatrix[,2])
}
# plot the means and standard errors of the training and test error with varying number of terms to fit
#  first, set up titles, etc.
mainTitle <- "Training and test error as a function of the number of terms"
trainTitle <- "Training error"
testTitle <- "Testing error"
ylab <- "RMSE"
xlab <- "N terms fit"
# find the min and max ranges for all data to plot on the same axes
ymin <- min(c(noiseTrainMatrix[,1]-noiseTrainMatrix[,2], noiseTestMatrix[,1]-noiseTestMatrix[,2]))
ymax <- max(c(noiseTrainMatrix[,1]+noiseTrainMatrix[,2], noiseTestMatrix[,1]+noiseTestMatrix[,2]))
buffer <- (ymax - ymin) * 0.1
ylim <- c(ymin-buffer, ymax+buffer)
# set up plot structure
par(mfcol=c(1,2), oma = c(0,0,2,0))
# set up the training data plot
plotCI(1:(ncol(data)-1), noiseTrainMatrix[,1], type = 'p', uiw = noiseTrainMatrix[,2], liw = noiseTrainMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'orange')
# set up the test data plot
plotCI(1:(ncol(data)-1), noiseTestMatrix[,1], type = 'p', uiw = noiseTestMatrix[,2], liw = noiseTestMatrix[,2], ylim = ylim,
pch = 19, ylab = ylab, xlab = xlab, main = trainTitle, col = 'black', barcol = 'aquamarine')
# and add a title
title(mainTitle, outer = TRUE)
